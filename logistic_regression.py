<<<<<<< HEAD
# # -*- coding: utf-8 -*-
# """Logistic Regression

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/15d7sfJe85jFDe_PBSPoKRcZgd-fELxul
# """

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

# Load dataset
file_path = "D:/Projects/ML Project/ML-Models-Comparison-Through-Customer-Churn-Prediction/Cleaned_E_Commerce_Data.csv"
df = pd.read_csv(file_path)

# Drop CustomerID as it is not a feature
X = df.drop(columns=['CustomerID', 'Churn'])
y = df['Churn']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc_roc = roc_auc_score(y_test, y_prob)

# Print results
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"AUC-ROC: {auc_roc:.4f}")

# Print Customer Churn Summary
print("\nCustomer Churn Summary:")
if recall > 0.75:
    print("High recall: The model successfully identifies most churners, allowing proactive retention efforts.")
elif recall < 0.5:
    print("Low recall: The model is missing many actual churners, which may lead to customer loss.")

if precision > 0.75:
    print("High precision: The model effectively predicts churners, minimizing false alarms and optimizing retention resources.")
elif precision < 0.5:
    print("Low precision: Many non-churners are incorrectly classified as churners, leading to wasted retention efforts.")

if auc_roc > 0.8:
    print("High AUC-ROC: The model effectively differentiates between churners and non-churners.")
elif auc_roc < 0.6:
    print("Low AUC-ROC: The model struggles to separate churners from non-churners.")

# Plot AUC-ROC curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'AUC-ROC Curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='grey', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('AUC-ROC Curve')
plt.legend()
plt.show()

# Plot bar graph of metrics
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']
values = [accuracy, precision, recall, f1, auc_roc]
plt.figure(figsize=(8, 6))
plt.bar(metrics, values, color=['blue', 'green', 'red', 'purple', 'orange'])
plt.ylim(0, 1)
plt.xlabel('Metrics')
plt.ylabel('Score')
plt.title('Model Performance Metrics')
=======
# # -*- coding: utf-8 -*-
# """Logistic Regression

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/15d7sfJe85jFDe_PBSPoKRcZgd-fELxul
# """

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

# Load dataset
file_path = "D:/Projects/ML Project/ML-Models-Comparison-Through-Customer-Churn-Prediction/Cleaned_E_Commerce_Data.csv"
df = pd.read_csv(file_path)

# Drop CustomerID as it is not a feature
X = df.drop(columns=['CustomerID', 'Churn'])
y = df['Churn']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc_roc = roc_auc_score(y_test, y_prob)

# Print results
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"AUC-ROC: {auc_roc:.4f}")

# Print Customer Churn Summary
print("\nCustomer Churn Summary:")
if recall > 0.75:
    print("High recall: The model successfully identifies most churners, allowing proactive retention efforts.")
elif recall < 0.5:
    print("Low recall: The model is missing many actual churners, which may lead to customer loss.")

if precision > 0.75:
    print("High precision: The model effectively predicts churners, minimizing false alarms and optimizing retention resources.")
elif precision < 0.5:
    print("Low precision: Many non-churners are incorrectly classified as churners, leading to wasted retention efforts.")

if auc_roc > 0.8:
    print("High AUC-ROC: The model effectively differentiates between churners and non-churners.")
elif auc_roc < 0.6:
    print("Low AUC-ROC: The model struggles to separate churners from non-churners.")

# Plot AUC-ROC curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'AUC-ROC Curve (AUC = {auc_roc:.4f})')
plt.plot([0, 1], [0, 1], color='grey', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('AUC-ROC Curve')
plt.legend()
plt.show()

# Plot bar graph of metrics
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']
values = [accuracy, precision, recall, f1, auc_roc]
plt.figure(figsize=(8, 6))
plt.bar(metrics, values, color=['blue', 'green', 'red', 'purple', 'orange'])
plt.ylim(0, 1)
plt.xlabel('Metrics')
plt.ylabel('Score')
plt.title('Model Performance Metrics')
>>>>>>> 6beea00e2bacdf6a04b11b2a917af4c0134bb444
plt.show()